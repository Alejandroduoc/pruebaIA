import os 
import sys
import time 
import pickle 
from typing import List, Dict, Any 
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings 
from langchain.schema import HumanMessage, SystemMessage, AIMessage # Importa las clases de mensajes para estructurar las conversaciones con el LLM.
from langchain.memory import ConversationBufferMemory # Importa la clase para gestionar la memoria de la conversaci√≥n.
from langchain.text_splitter import RecursiveCharacterTextSplitter # Importa la clase para dividir textos largos en fragmentos (chunks).
from langchain_community.vectorstores import FAISS 
from langchain.docstore.document import Document 

class ChatbotEvaluacion: # Define la clase principal del chatbot.
    def __init__(self): # Define el m√©todo constructor que se ejecuta al crear una instancia de la clase.
        # Configuraci√≥n del modelo de lenguaje (LLM) con streaming.
        self.llm = ChatOpenAI(
            base_url="https://models.github.ai/inference",  # URL base de la API de modelos de GitHub.
            api_key=os.getenv("GITHUB_TOKEN"),     # Obtiene la clave de API desde las variables de entorno.
            model="openai/gpt-4o-mini", # Especifica el modelo a utilizar.
            temperature=0.7, # Define la "creatividad" del modelo (valores m√°s altos son m√°s creativos).
            streaming=True # Habilita el modo de streaming para recibir la respuesta token por token.
        )
        
        # Configuraci√≥n del modelo de embeddings.
        self.embeddings = OpenAIEmbeddings(
            base_url="https://models.github.ai/inference", # URL base para el servicio de embeddings.
            api_key=os.getenv("GITHUB_TOKEN") # Usa la misma clave de API.
        )
        
        # Inicializaci√≥n del sistema de memoria para la conversaci√≥n.
        self.memory = ConversationBufferMemory(
            return_messages=True, # Configura la memoria para que devuelva los mensajes como objetos (HumanMessage, AIMessage).
            memory_key="chat_history" # Clave bajo la cual se guardar√° el historial en la memoria.
        )
        
        # Inicializaci√≥n de la base de datos vectorial (inicialmente vac√≠a).
        self.vectorstore = None
        # Configuraci√≥n del divisor de texto.
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, # Tama√±o m√°ximo de cada fragmento (chunk) en caracteres.
            chunk_overlap=200 # N√∫mero de caracteres que se solapan entre chunks consecutivos para mantener el contexto.
        )
        
        # El RAG (Retrieval-Augmented Generation) se inicializar√° m√°s tarde a trav√©s de la interfaz de usuario.
    
    def inicializar_rag(self, docs: List[Document]): # Define un m√©todo para inicializar el sistema RAG.
        """Inicializa el sistema RAG con los documentos proporcionados"""
        if not docs: # Comprueba si la lista de documentos est√° vac√≠a.
            st.error("No se proporcionaron documentos para inicializar RAG.") # Muestra un error en la UI si no hay documentos.
            return # Termina la ejecuci√≥n del m√©todo.
            
        st.info("üìö Inicializando sistema RAG...") # Muestra un mensaje informativo en la UI.
        
        # Divide los documentos en fragmentos (chunks) usando el divisor de texto configurado.
        chunks = self.text_splitter.split_documents(docs)
        st.success(f"‚úì Divididos {len(chunks)} chunks") # Muestra un mensaje de √©xito con el n√∫mero de chunks creados.
        
        # Genera los embeddings para cada chunk y crea la base de datos vectorial FAISS.
        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        st.success("‚úì Base de datos vectorial FAISS creada") # Muestra un mensaje de √©xito.
        
        # Guarda la base de datos vectorial en el disco local.
        self.vectorstore.save_local("faiss_index")
        st.success("‚úì Base de datos guardada en 'faiss_index'") # Muestra un mensaje de √©xito.
    
    def cargar_rag(self, ruta: str = "faiss_index"): # Define un m√©todo para cargar una base de datos RAG existente.
        """Carga una base de datos vectorial existente"""
        if os.path.exists(ruta): # Comprueba si la ruta especificada existe en el disco.
            # Carga la base de datos FAISS desde el disco, usando el modelo de embeddings configurado.
            # allow_dangerous_deserialization es necesario por seguridad en versiones recientes de LangChain.
            self.vectorstore = FAISS.load_local(ruta, self.embeddings, allow_dangerous_deserialization=True)
            st.success("‚úì Base de datos vectorial cargada") # Muestra un mensaje de √©xito.
            return True # Devuelve True si la carga fue exitosa.
        return False # Devuelve False si la ruta no existe.
    
    def buscar_contexto(self, pregunta: str, k: int = 3) -> str: # Define un m√©todo para buscar contexto relevante.
        """Busca contexto relevante en la base de datos vectorial"""
        if not self.vectorstore: # Comprueba si la base de datos vectorial ha sido inicializada.
            return "Sistema RAG no inicializado. Por favor, carga documentos primero." # Devuelve un mensaje de advertencia.
        
        # Realiza una b√∫squeda por similitud en la base de datos vectorial.
        docs = self.vectorstore.similarity_search(pregunta, k=k) # 'k' es el n√∫mero de documentos m√°s relevantes a recuperar.
        # Une el contenido de los documentos recuperados en un solo string.
        contexto = "\n\n".join([doc.page_content for doc in docs])
        return f"Contexto relevante:\n{contexto}" # Devuelve el contexto formateado.
    
    def generar_respuesta_con_streaming(self, pregunta: str, mostrar_contexto: bool = False): # Define el m√©todo principal para generar respuestas.
        """Genera respuesta con streaming integrando RAG y memoria"""
        
        # Busca contexto relevante en la base de datos vectorial si est√° disponible.
        contexto = self.buscar_contexto(pregunta) if self.vectorstore else ""
        
        # Carga el historial de la conversaci√≥n desde la memoria.
        historial = self.memory.load_memory_variables({})["chat_history"]
        
        # Construye el prompt del sistema que instruye al LLM sobre su rol y comportamiento.
        system_prompt = """
Eres un asistente de soporte inform√°tico experto para oficinas. 
Tu conocimiento se basa **exclusivamente** en el material proporcionado en el archivo 'soporte_informatica.txt'.

Directrices:
1. Responde de manera clara, concisa y profesional.
2. Solo utiliza la informaci√≥n contenida en el archivo; **no inventes respuestas ni uses conocimiento externo**.
3. Si la pregunta no puede ser respondida con la informaci√≥n disponible, responde amablemente:
   "La informaci√≥n para responder a tu pregunta no se encuentra en el material disponible."
4. Siempre prioriza la seguridad inform√°tica y las buenas pr√°cticas de oficina.
5. Para problemas t√©cnicos, proporciona pasos pr√°cticos y ordenados.
6. Mant√©n un tono amigable y de ayuda.

Formato:
- Si es un procedimiento paso a paso, enumera claramente los pasos.
- Para informaci√≥n de contacto o horarios, incl√∫yelos tal como aparecen en el material.
"""

        
        # Inicializa la lista de mensajes con el prompt del sistema.
        messages = [SystemMessage(content=system_prompt)]
        
        # Agrega el historial de la conversaci√≥n a la lista de mensajes.
        for msg in historial:
            messages.append(msg)
        
        # Si se debe mostrar el contexto, lo agrega como un mensaje del sistema.
        if contexto and mostrar_contexto:
            messages.append(SystemMessage(content=contexto))
        
        # Agrega la pregunta actual del usuario como un mensaje humano.
        messages.append(HumanMessage(content=pregunta))
        
        # Inicia la generaci√≥n de respuesta con streaming.
        respuesta_completa = "" # Variable para almacenar la respuesta completa.
        try:
            # Crea un contenedor vac√≠o en Streamlit que se actualizar√° din√°micamente.
            respuesta_placeholder = st.empty()
            respuesta_texto = "ü§ñ **Asistente:** " # Texto inicial de la respuesta.
            
            # Itera sobre los chunks de respuesta que llegan desde el LLM en modo streaming.
            for chunk in self.llm.stream(messages):
                contenido = chunk.content # Obtiene el contenido del chunk actual.
                respuesta_texto += contenido # Agrega el contenido al texto de la respuesta.
                respuesta_placeholder.markdown(respuesta_texto + "‚ñå") # Actualiza el placeholder con el texto y un cursor parpadeante.
                respuesta_completa += contenido # Acumula el contenido en la variable de respuesta completa.
                time.sleep(0.02) # Peque√±a pausa para un efecto visual m√°s suave.
            
            # Muestra la respuesta final en el placeholder sin el cursor.
            respuesta_placeholder.markdown(respuesta_texto)
            
            # Guarda el par pregunta/respuesta en la memoria para mantener el contexto de la conversaci√≥n.
            self.memory.save_context(
                {"input": pregunta},
                {"output": respuesta_completa}
            )
            
            return respuesta_completa # Devuelve la respuesta completa generada.
            
        except Exception as e: # Maneja posibles errores durante la generaci√≥n.
            error_msg = f"‚ùå Error en la generaci√≥n: {e}" # Formatea el mensaje de error.
            st.error(error_msg) # Muestra el error en la interfaz de Streamlit.
            return error_msg # Devuelve el mensaje de error.
    
    def evaluar_respuesta(self, pregunta: str, respuesta_usuario: str) -> Dict[str, Any]: # Define un m√©todo para evaluar la respuesta de un usuario.
        """Eval√∫a una respuesta del usuario compar√°ndola con el contexto"""
        if not self.vectorstore: # Comprueba si el sistema RAG est√° inicializado.
            return {"error": "Sistema RAG no inicializado"} # Devuelve un error si no lo est√°.
        
        # Busca el contexto relevante para la pregunta.
        contexto = self.buscar_contexto(pregunta)
        
        # Crea un prompt espec√≠fico para que el LLM eval√∫e la respuesta del estudiante.
        prompt_evaluacion = f'''
        Eval√∫a la siguiente respuesta del estudiante:
        
        Pregunta: {pregunta}
        Respuesta del estudiante: {respuesta_usuario}
        
        Contexto de referencia: {contexto}
        
        Proporciona una evaluaci√≥n con:
        1. Puntuaci√≥n (0-10)
        2. Puntos fuertes
        3. √Åreas de mejora
        4. Explicaci√≥n breve
        '''
        
        try:
            # Llama al LLM con el prompt de evaluaci√≥n.
            respuesta = self.llm.invoke([HumanMessage(content=prompt_evaluacion)])
            # Devuelve un diccionario con el feedback y el contexto utilizado.
            return {
                "puntuacion": 0,  # Placeholder, idealmente se extraer√≠a del 'feedback' con parsing.
                "feedback": respuesta.content, # El feedback generado por el LLM.
                "contexto_utilizado": contexto[:500] + "..." if len(contexto) > 500 else contexto # Muestra una parte del contexto.
            }
        except Exception as e: # Maneja errores durante la evaluaci√≥n.
            return {"error": f"Error en evaluaci√≥n: {e}"}
    
    def mostrar_estadisticas(self): # Define un m√©todo para mostrar estad√≠sticas.
        """Muestra estad√≠sticas del chatbot"""
        # Carga el historial de la conversaci√≥n desde la memoria.
        historial = self.memory.load_memory_variables({})["chat_history"]
        st.subheader("üìä Estad√≠sticas del Chatbot") # T√≠tulo para la secci√≥n de estad√≠sticas.
        st.write(f"üí¨ **Mensajes en memoria:** {len(historial)}") # Muestra el n√∫mero de mensajes en memoria.
        st.write(f"üìö **Base vectorial:** {'‚úì Cargada' if self.vectorstore else '‚úó No disponible'}") # Indica si la base vectorial est√° cargada.
        if self.vectorstore: # Si la base vectorial existe...
            st.write(f"üìñ **Documentos indexados:** {self.vectorstore.index.ntotal}") # Muestra el n√∫mero de documentos (chunks) indexados.

# FUNCIONES DE UTILIDAD PARA LA EVALUACI√ìN

def cargar_material() -> List[Document]: # Define una funci√≥n para cargar los archivos de material de estudio.
    """Carga el material de estudio de los m√≥dulos RA1 desde archivos markdown."""
    st.info("Cargando material de estudio de RA1...") # Mensaje informativo en la UI.
    documentos = [] # Inicializa una lista vac√≠a para almacenar los documentos.
    # Define un diccionario con nombres l√≥gicos y rutas relativas a los archivos.
    rutas_archivos = {
        "soporte_informatica": "soporte_informatica.txt",
    }
    
    # Obtiene la ruta absoluta del directorio donde se encuentra el script actual.
    dir_actual = os.path.dirname(os.path.abspath(__file__))

    # Itera sobre el diccionario de rutas de archivos.
    for nombre, ruta_relativa in rutas_archivos.items():
        try:
            # Construye la ruta absoluta del archivo.
            ruta_absoluta = os.path.normpath(os.path.join(dir_actual, ruta_relativa))
            # Abre y lee el archivo con codificaci√≥n UTF-8.
            with open(ruta_absoluta, 'r', encoding='utf-8') as f:
                contenido = f.read() # Lee todo el contenido del archivo.
                # L√≥gica especial para el archivo de Evaluaciones: solo carga hasta la formativa 2.
                if "Evaluaciones" in nombre:
                    if "## EVALUACI√ìN FORMATIVA 2" in contenido:
                        contenido = contenido.split("## EVALUACI√ìN FORMATIVA 2")[0] # Trunca el contenido.

                # Crea un objeto Document de LangChain con el contenido y metadatos.
                documentos.append(Document(
                    page_content=contenido,
                    metadata={"source": nombre, "file_path": ruta_relativa}
                ))
            st.write(f"‚úì Cargado: {nombre}") # Mensaje de √©xito por cada archivo cargado.
        except FileNotFoundError: # Maneja el error si un archivo no se encuentra.
            st.warning(f"‚ö†Ô∏è No se encontr√≥ el archivo: {ruta_absoluta}")
        except Exception as e: # Maneja cualquier otro error durante la carga.
            st.error(f"‚ùå Error al cargar {ruta_absoluta}: {e}")
            
    if not documentos: # Si no se carg√≥ ning√∫n documento...
        st.error("No se pudo cargar ning√∫n material de estudio. El chatbot no tendr√° contexto.") # Muestra un error.
    
    return documentos # Devuelve la lista de objetos Document.

def main(): # Funci√≥n principal que define la aplicaci√≥n Streamlit.
    st.set_page_config( # Configura la p√°gina de la aplicaci√≥n.
        page_title="Asistente de Soporte en informatica", # T√≠tulo que aparece en la pesta√±a del navegador.
        page_icon="ü§ñ", # √çcono de la p√°gina.
        layout="wide" # Utiliza un dise√±o de p√°gina ancho.
    )
    
    st.title("ü§ñ Asistente de Soporte en informatica") # T√≠tulo principal de la aplicaci√≥n.
    st.markdown("Usa este chatbot para preguntar sobre Soporte en informatica.") # Subt√≠tulo o descripci√≥n.
    
    # Inicializa el chatbot en el estado de la sesi√≥n de Streamlit si no existe.
    # Esto asegura que el objeto chatbot persista entre interacciones del usuario.
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = ChatbotEvaluacion()
    
    # Crea una barra lateral para las opciones de configuraci√≥n.
    with st.sidebar:
        st.header("Configuraci√≥n") # T√≠tulo de la barra lateral.
        
        # Bot√≥n para cargar el material de estudio.
        if st.button("üìö Cargar material de Soporte en informatica"):
            docs = cargar_material() # Llama a la funci√≥n para cargar los archivos.
            if docs: # Si se cargaron documentos...
                st.session_state.chatbot.inicializar_rag(docs) # Inicializa el RAG con los documentos.
        
        # Bot√≥n para cargar una base de datos vectorial existente.
        if st.button("üîç Cargar base de datos existente"):
            if st.session_state.chatbot.cargar_rag(): # Intenta cargar la base de datos.
                st.success("Base de datos cargada exitosamente") # Mensaje de √©xito.
            else:
                st.error("No se encontr√≥ base de datos existente") # Mensaje de error.
        
        # Bot√≥n para mostrar estad√≠sticas del chatbot.
        if st.button("üìä Mostrar estad√≠sticas"):
            st.session_state.chatbot.mostrar_estadisticas() # Llama al m√©todo para mostrar estad√≠sticas.
        
        # Bot√≥n para limpiar la memoria de la conversaci√≥n.
        if st.button("üóëÔ∏è Limpiar memoria"):
            st.session_state.chatbot.memory.clear() # Limpia la memoria.
            st.success("Memoria limpiada") # Mensaje de √©xito.
        
        st.markdown("---") # L√≠nea divisoria.
        st.markdown("### Modos de uso") # Subt√≠tulo para los modos.
        # Botones de radio para seleccionar el modo de operaci√≥n.
        modo = st.radio("Selecciona el modo:", 
                       ["üí¨ Chat", "üìù Evaluaci√≥n"])
    
    # L√≥gica para el contenido principal seg√∫n el modo seleccionado.
    if modo == "üí¨ Chat":
        st.header("üí¨ Modo Chat") # T√≠tulo para el modo chat.
        
        # Muestra el historial de chat si existe.
        if hasattr(st.session_state.chatbot, 'memory'):
            historial = st.session_state.chatbot.memory.load_memory_variables({})["chat_history"]
            if historial:
                with st.expander("üìú Historial de conversaci√≥n"): # Crea una secci√≥n expandible.
                    for i, msg in enumerate(historial): # Itera sobre los mensajes del historial.
                        if isinstance(msg, HumanMessage): # Si es un mensaje de usuario...
                            st.write(f"**üßë Usuario:** {msg.content}")
                        elif isinstance(msg, AIMessage): # Si es un mensaje del asistente...
                            st.write(f"**ü§ñ Asistente:** {msg.content}")
        
        # √Årea de texto para que el usuario ingrese su pregunta.
        pregunta = st.text_area("Escribe tu pregunta:", placeholder="¬øEn qu√© puedo ayudarte?")
        
        col1, col2 = st.columns([1, 5]) # Divide el espacio en dos columnas.
        with col1:
            # Checkbox para decidir si se muestra el contexto recuperado por RAG.
            mostrar_contexto = st.checkbox("Mostrar contexto", value=False)
        with col2:
            # Bot√≥n para enviar la pregunta.
            if st.button("üöÄ Enviar pregunta", type="primary"):
                if pregunta.strip(): # Comprueba que la pregunta no est√© vac√≠a.
                    with st.spinner("Generando respuesta..."): # Muestra un indicador de carga.
                        # Llama al m√©todo para generar la respuesta.
                        st.session_state.chatbot.generar_respuesta_con_streaming(
                            pregunta, 
                            mostrar_contexto=mostrar_contexto
                        )
                else:
                    st.warning("Por favor, escribe una pregunta") # Advierte si la pregunta est√° vac√≠a.
    
    elif modo == "üìù Evaluaci√≥n":
        st.header("üìù Modo Evaluaci√≥n") # T√≠tulo para el modo evaluaci√≥n.
        
        st.markdown("Eval√∫a una respuesta del estudiante compar√°ndola con el contexto disponible.") # Descripci√≥n.
        
        col1, col2 = st.columns(2) # Divide el espacio en dos columnas.
        
        with col1:
            # √Årea de texto para la pregunta de evaluaci√≥n.
            pregunta_eval = st.text_area(
                "Pregunta de evaluaci√≥n:",
                placeholder="Ej: ¬øQu√© es la inteligencia artificial?",
                height=100
            )
        
        with col2:
            # √Årea de texto para la respuesta del estudiante.
            respuesta_estudiante = st.text_area(
                "Respuesta del estudiante:",
                placeholder="Ej: La IA es cuando las m√°quinas piensan como humanos",
                height=100
            )
        
        # Bot√≥n para iniciar la evaluaci√≥n.
        if st.button("üìã Evaluar respuesta", type="primary"):
            if pregunta_eval.strip() and respuesta_estudiante.strip(): # Comprueba que ambos campos est√©n llenos.
                with st.spinner("Evaluando respuesta..."): # Muestra un indicador de carga.
                    # Llama al m√©todo de evaluaci√≥n.
                    evaluacion = st.session_state.chatbot.evaluar_respuesta(
                        pregunta_eval, 
                        respuesta_estudiante
                    )
                    
                    if "error" not in evaluacion: # Si no hubo error en la evaluaci√≥n...
                        st.subheader("Resultado de la evaluaci√≥n") # Muestra el resultado.
                        st.markdown(evaluacion["feedback"]) # Muestra el feedback del LLM.
                        
                        with st.expander("üìö Contexto utilizado"): # Secci√≥n expandible para el contexto.
                            st.text(evaluacion["contexto_utilizado"])
                    else:
                        st.error(evaluacion["error"]) # Muestra el error si ocurri√≥ uno.
            else:
                st.warning("Por favor, completa ambos campos") # Advierte si faltan campos.

    # Pie de p√°gina de la aplicaci√≥n.
    st.markdown("---")
    st.markdown("*Asistente de estudio con RAG y memoria de conversaci√≥n para RA1*")

if __name__ == "__main__": # Punto de entrada del script. Se ejecuta solo si el archivo es corrido directamente.
    # Verifica si la variable de entorno GITHUB_TOKEN est√° configurada.
    if not os.getenv("GITHUB_TOKEN"):
        st.error("‚ö†Ô∏è Por favor, configura la variable de entorno GITHUB_TOKEN") # Muestra un error si no est√° configurada.
        st.info("""
        Para usar esta aplicaci√≥n, necesitas:
        1. Obtener un token de GitHub con permisos adecuados
        2. Configurarlo como variable de entorno:
           ```bash
           export GITHUB_TOKEN="tu_token_aqui"
           ```
        """)
    else:
        main() # Llama a la funci√≥n principal para iniciar la aplicaci√≥n.