{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bmsvkxtyalo",
   "metadata": {},
   "source": [
    "# 4. LangChain Memory - Gestión de Contexto Conversacional\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender la importancia de la memoria en conversaciones con LLMs\n",
    "- Implementar diferentes tipos de memoria con LangChain\n",
    "- Gestionar el contexto de conversaciones largas\n",
    "- Optimizar el uso de tokens con estrategias de memoria\n",
    "\n",
    "## ¿Por qué es Importante la Memoria?\n",
    "\n",
    "Los LLMs son **stateless** por naturaleza: no recuerdan conversaciones anteriores. La memoria permite:\n",
    "- **Contexto conversacional**: Referirse a mensajes anteriores\n",
    "- **Personalización**: Recordar preferencias del usuario\n",
    "- **Continuidad**: Mantener hilos de conversación coherentes\n",
    "- **Experiencia natural**: Conversaciones que se sienten humanas\n",
    "\n",
    "## Tipos de Memoria en LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Mantiene todo el historial\n",
    "2. **ConversationSummaryMemory**: Resume conversaciones largas\n",
    "3. **ConversationBufferWindowMemory**: Mantiene solo los N mensajes más recientes\n",
    "4. **ConversationSummaryBufferMemory**: Combina resumen + buffer reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias para memoria\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "import os\n",
    "\n",
    "print(\"✓ Bibliotecas de memoria importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo para memoria\n",
    "try:\n",
    "    \"\"\" llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0.7\n",
    "    ) \"\"\"\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"https://models.github.ai/inference\",  #descubri que aqui  cambia de url base  \n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),               \n",
    "        model = \"openai/gpt-4.1\",               # modelo DeepSeek\n",
    "        temperature=0.7,\n",
    "        # max_tokens=150\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Modelo configurado para experimentos de memoria\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error en configuración: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc98272",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory - Memoria Completa\n",
    "\n",
    "Esta memoria mantiene **todo** el historial de la conversación. Es la más simple pero puede consumir muchos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fb2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATIONBUFFERMEMORY ===\n",
      "Mantiene todo el historial de conversación\\n\n",
      "1. Primera pregunta:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Mi nombre es Pedro y soy programadora C#\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta: ¡Hola, Pedro! Es un gusto conocerte. Qué chévere que seas programadora en C#. ¿En qué tipo de proyectos has trabajado? C# es muy versátil: desde aplicaciones de escritorio con Windows Forms o WPF, hasta desarrollo web con ASP.NET Core, y hasta juegos con Unity. Si tienes alguna pregunta sobre C# o quieres compartir algún reto que hayas tenido, ¡cuéntame!\\n\n",
      "2. Segunda pregunta:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mi nombre es Pedro y soy programadora C#\n",
      "AI: ¡Hola, Pedro! Es un gusto conocerte. Qué chévere que seas programadora en C#. ¿En qué tipo de proyectos has trabajado? C# es muy versátil: desde aplicaciones de escritorio con Windows Forms o WPF, hasta desarrollo web con ASP.NET Core, y hasta juegos con Unity. Si tienes alguna pregunta sobre C# o quieres compartir algún reto que hayas tenido, ¡cuéntame!\n",
      "Human: ¿Cuál es mi nombre y profesión?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta: Tu nombre es Pedro y eres programadora en C#.\\n\n",
      "3. Tercera pregunta:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Mi nombre es Pedro y soy programadora C#\n",
      "AI: ¡Hola, Pedro! Es un gusto conocerte. Qué chévere que seas programadora en C#. ¿En qué tipo de proyectos has trabajado? C# es muy versátil: desde aplicaciones de escritorio con Windows Forms o WPF, hasta desarrollo web con ASP.NET Core, y hasta juegos con Unity. Si tienes alguna pregunta sobre C# o quieres compartir algún reto que hayas tenido, ¡cuéntame!\n",
      "Human: ¿Cuál es mi nombre y profesión?\n",
      "AI: Tu nombre es Pedro y eres programadora en C#.\n",
      "Human: ¿Qué lenguaje de programación mencioné?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta: Mencionaste el lenguaje de programación C#.\\n\n",
      "=== CONTENIDO DE LA MEMORIA ===\n",
      "Human: Mi nombre es Pedro y soy programadora C#\n",
      "AI: ¡Hola, Pedro! Es un gusto conocerte. Qué chévere que seas programadora en C#. ¿En qué tipo de proyectos has trabajado? C# es muy versátil: desde aplicaciones de escritorio con Windows Forms o WPF, hasta desarrollo web con ASP.NET Core, y hasta juegos con Unity. Si tienes alguna pregunta sobre C# o quieres compartir algún reto que hayas tenido, ¡cuéntame!\n",
      "Human: ¿Cuál es mi nombre y profesión?\n",
      "AI: Tu nombre es Pedro y eres programadora en C#.\n",
      "Human: ¿Qué lenguaje de programación mencioné?\n",
      "AI: Mencionaste el lenguaje de programación C#.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo básico con ConversationBufferMemory\n",
    "def ejemplo_buffer_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERMEMORY ===\")\n",
    "    print(\"Mantiene todo el historial de conversación\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria buffer\n",
    "        memory = ConversationBufferMemory()\n",
    "        \n",
    "        # Crear cadena de conversación\n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True  # Muestra el prompt interno\n",
    "        )\n",
    "        \n",
    "        # Primera interacción\n",
    "        print(\"1. Primera pregunta:\")\n",
    "        response1 = conversation.predict(input=\"Mi nombre es Pedro y soy programadora C#\")\n",
    "        print(f\"Respuesta: {response1}\\\\n\")\n",
    "        \n",
    "        # Segunda interacción (debe recordar el nombre)\n",
    "        print(\"2. Segunda pregunta:\")\n",
    "        response2 = conversation.predict(input=\"¿Cuál es mi nombre y profesión?\")\n",
    "        print(f\"Respuesta: {response2}\\\\n\")\n",
    "        \n",
    "        # Tercera interacción (debe recordar todo el contexto)\n",
    "        print(\"3. Tercera pregunta:\")\n",
    "        response3 = conversation.predict(input=\"¿Qué lenguaje de programación mencioné?\")\n",
    "        print(f\"Respuesta: {response3}\\\\n\")\n",
    "        \n",
    "        # Examinar el contenido de la memoria\n",
    "        print(\"=== CONTENIDO DE LA MEMORIA ===\")\n",
    "        print(memory.buffer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_buffer_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1acba3",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory - Ventana Deslizante\n",
    "\n",
    "Esta memoria mantiene solo los **N mensajes más recientes**, útil para controlar el uso de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46248c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONVERSATIONSUMMARYMEMORY ===\n",
      "Resume conversaciones largas para ahorrar tokens\\n\n",
      "1. Input: Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo web con React y Node.js. Trabajo en una startup de fintech en Madrid desde hace 3 años.\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_4364\\1771842198.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(llm=llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   Respuesta: ¡Hola, María! Encantada de conocerte. Qué interesante tu perfil profesional: React y Node.js son tecnologías súper demandadas hoy en día, especialmente en el sector fintech, donde la agilidad y la escalabilidad son clave. Madrid tiene una comunidad tech muy activa, y las startups fintech están revolucionando el mundo financiero con soluciones innovadoras.\n",
      "\n",
      "Si te apetece, cuéntame más sobre tu trabajo: ¿en qué tipo de proyectos te sueles involucrar? ¿Utilizáis otras tecnologías en tu stack, como TypeScript, GraphQL o herramientas de CI/CD? También, si tienes alguna pregunta sobre desarrollo web, tendencias en fintech, o incluso sobre cómo mejorar procesos dentro de tu equipo, estaré encantada de ayudarte.\\n\n",
      "   Resumen actual en memoria:\n",
      "   María González, una ingeniera de software especializada en desarrollo web con React y Node.js, comparte que trabaja en una startup fintech en Madrid desde hace 3 años. El AI destaca el valor de su perfil profesional y el papel innovador de las fintech en Madrid, preguntando sobre los proyectos en los que María participa, las tecnologías que utiliza y ofreciéndose para responder dudas sobre desarrollo web, tendencias fintech o mejoras de procesos de equipo.\\n\n",
      "--------------------------------------------------------------------------------\n",
      "2. Input: Mi proyecto actual involucra crear una plataforma de pagos digitales que debe manejar transacciones ...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "María González, una ingeniera de software especializada en desarrollo web con React y Node.js, comparte que trabaja en una startup fintech en Madrid desde hace 3 años. El AI destaca el valor de su perfil profesional y el papel innovador de las fintech en Madrid, preguntando sobre los proyectos en los que María participa, las tecnologías que utiliza y ofreciéndose para responder dudas sobre desarrollo web, tendencias fintech o mejoras de procesos de equipo.\n",
      "Human: Mi proyecto actual involucra crear una plataforma de pagos digitales que debe manejar transacciones en tiempo real con alta seguridad. Usamos microservicios con Docker y Kubernetes.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   Respuesta: ¡Qué emocionante proyecto, María! Crear una plataforma de pagos digitales en tiempo real presenta retos súper interesantes, especialmente en lo que respecta a la escalabilidad, la seguridad y la resiliencia del sistema.\n",
      "\n",
      "El enfoque de microservicios con Docker y Kubernetes es ideal para este tipo de soluciones:  \n",
      "- **Docker** permite aislar cada servicio (por ejemplo, gestión de usuarios, procesamiento de pagos, notificaciones) en contenedores independientes, facilitando el despliegue y la actualización.\n",
      "- **Kubernetes** se encarga de la orquestación, asegurando alta disponibilidad, balanceo de carga y recuperación automática ante fallos.\n",
      "\n",
      "Para plataformas de pagos, algunos puntos clave suelen ser:  \n",
      "1. **Seguridad**:  \n",
      "   - Implementar cifrado fuerte (TLS para comunicaciones, cifrado en reposo en la base de datos).\n",
      "   - Autenticación y autorización robustas, idealmente con OAuth2 y JWT.\n",
      "   - Monitoreo de anomalías y detección de fraude en tiempo real.\n",
      "\n",
      "2. **Transacciones en Tiempo Real**:  \n",
      "   - Uso de colas/eventos (Kafka, RabbitMQ) para procesar pagos y notificaciones sin bloqueos.\n",
      "   - Bases de datos que soporten alta concurrencia y consistencia (por ejemplo, PostgreSQL, MongoDB, o incluso soluciones más especializadas como CockroachDB).\n",
      "\n",
      "3. **Escalabilidad y Alta Disponibilidad**:  \n",
      "   - Despliegues automáticos, escalado horizontal con Kubernetes.\n",
      "   - Pruebas de resiliencia y stress testing (Chaos Engineering puede ser útil).\n",
      "\n",
      "¿Utilizan algún proveedor de pagos externo (como Stripe, Adyen) o están construyendo la integración directamente con bancos?  \n",
      "¿Han implementado algún sistema para la observabilidad, como Prometheus, Grafana o ELK Stack?  \n",
      "Si tienes dudas específicas sobre la arquitectura, seguridad, o cómo optimizar el trabajo en equipo en este entorno, ¡dímelo y te ayudo encantado!\\n\n",
      "   Resumen actual en memoria:\n",
      "   María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\\n\n",
      "--------------------------------------------------------------------------------\n",
      "3. Input: El mayor desafío técnico que enfrentamos es la latencia en las transacciones internacionales. Estamo...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\n",
      "Human: El mayor desafío técnico que enfrentamos es la latencia en las transacciones internacionales. Estamos considerando implementar edge computing y optimizar nuestras APIs.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   Respuesta: ¡Qué interesante, María! La latencia en transacciones internacionales es un reto común en plataformas de pagos digitales, especialmente cuando se manejan volúmenes altos y se requiere respuesta en tiempo real. Tu enfoque de considerar edge computing y optimización de APIs es muy acertado; te comparto algunos detalles y recomendaciones:\n",
      "\n",
      "**1. Edge computing en pagos internacionales:**  \n",
      "Implementar nodos de edge cerca de tus usuarios puede reducir significativamente la latencia, ya que los datos y procesos críticos (como validaciones preliminares o autenticación) ocurren más cerca del origen de la transacción. Algunas estrategias incluyen:\n",
      "- **Despliegue de microservicios críticos en regiones clave** a través de proveedores como AWS Lambda@Edge, Cloudflare Workers o Azure Edge Zones.\n",
      "- **Caching inteligente de datos no sensibles** en el edge para evitar viajes innecesarios al backend central.\n",
      "- **Balanceo de cargas geográfico** para redirigir peticiones al nodo más cercano.\n",
      "\n",
      "**2. Optimización de APIs:**  \n",
      "La velocidad de las APIs es fundamental. Algunas prácticas recomendadas:\n",
      "- **Usar HTTP/2 o gRPC** para mejorar la eficiencia en la comunicación, especialmente en conexiones internacionales.\n",
      "- **Reducir el tamaño de las respuestas** (evita sobrecarga de datos no necesarios, usa compresión).\n",
      "- **Implementar paginación y filtros** para minimizar la cantidad de datos transferidos.\n",
      "- **Monitorear y analizar rutas lentas** con herramientas como New Relic, Datadog o Prometheus+Grafana para identificar cuellos de botella.\n",
      "\n",
      "**3. Otras consideraciones técnicas:**  \n",
      "- **Optimiza la gestión de conexiones** (por ejemplo, usando pools y reintentos inteligentes).\n",
      "- **Evalúa el uso de CDN para assets estáticos** y, si tu arquitectura lo permite, para ciertas respuestas API cacheables.\n",
      "- **Revisa el procesamiento de datos en tiempo real**: ¿usan colas de mensajes como Kafka o RabbitMQ para desacoplar y agilizar procesos críticos?\n",
      "\n",
      "**4. En el contexto fintech:**  \n",
      "La seguridad sigue siendo prioritaria. Si procesas datos en el edge, asegúrate de cumplir con normativas (PCI DSS, GDPR) y de implementar cifrado de extremo a extremo.\n",
      "\n",
      "¿Han identificado exactamente en qué parte del flujo ocurre la mayor latencia (autenticación, procesamiento, comunicación con bancos o proveedores)? Si tienes métricas o trazas, podemos analizarlas y buscar soluciones más específicas.\n",
      "\n",
      "¡Cuéntame más sobre su arquitectura actual y en qué etapa están evaluando estas mejoras! Si lo deseas, puedo ayudarte a comparar opciones de edge providers o a diseñar pruebas de rendimiento para sus APIs.\\n\n",
      "   Resumen actual en memoria:\n",
      "   María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\n",
      "\n",
      "María menciona que el mayor desafío técnico actual es la latencia en transacciones internacionales, por lo que están considerando edge computing y optimización de APIs. El AI reconoce la importancia de estos enfoques y recomienda estrategias técnicas como el despliegue de microservicios críticos en nodos edge regionales, caching inteligente, balanceo de carga geográfico, uso de HTTP/2 o gRPC, compresión, paginación, monitoreo de rutas lentas y optimización de conexiones. También sugiere revisar el procesamiento en tiempo real con colas de mensajes, el uso de CDN y el cumplimiento normativo en el edge. El AI pregunta en qué parte específica del flujo ocurre la mayor latencia y ofrece ayuda para analizar métricas, comparar proveedores de edge y diseñar pruebas de rendimiento para APIs.\\n\n",
      "--------------------------------------------------------------------------------\n",
      "4. Input: También estoy trabajando en mejorar la experiencia de usuario de nuestra aplicación móvil. Los usuar...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\n",
      "\n",
      "María menciona que el mayor desafío técnico actual es la latencia en transacciones internacionales, por lo que están considerando edge computing y optimización de APIs. El AI reconoce la importancia de estos enfoques y recomienda estrategias técnicas como el despliegue de microservicios críticos en nodos edge regionales, caching inteligente, balanceo de carga geográfico, uso de HTTP/2 o gRPC, compresión, paginación, monitoreo de rutas lentas y optimización de conexiones. También sugiere revisar el procesamiento en tiempo real con colas de mensajes, el uso de CDN y el cumplimiento normativo en el edge. El AI pregunta en qué parte específica del flujo ocurre la mayor latencia y ofrece ayuda para analizar métricas, comparar proveedores de edge y diseñar pruebas de rendimiento para APIs.\n",
      "Human: También estoy trabajando en mejorar la experiencia de usuario de nuestra aplicación móvil. Los usuarios se quejan de que el proceso de verificación de identidad es muy lento.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   Respuesta: ¡Qué buen enfoque, María! La verificación de identidad (“onboarding” o KYC) es crítica en fintech, pero si es lenta, puede afectar la conversión y frustrar a los usuarios. Para mejorar la experiencia, aquí tienes varias ideas y detalles prácticos:\n",
      "\n",
      "**1. Análisis del flujo actual:**  \n",
      "- ¿Dónde ocurre la mayor demora? ¿En la captura de documentos, el procesamiento backend, o la respuesta del proveedor externo de verificación?\n",
      "- Revisa logs y métricas de tiempo entre cada paso (por ejemplo, desde que el usuario sube su foto hasta que recibe el resultado).\n",
      "\n",
      "**2. Optimización técnica:**  \n",
      "- **Carga y procesamiento local:** Si la app permite capturar fotos/documentos, asegúrate de comprimir y optimizar las imágenes antes de enviarlas. Puede hacerse en el móvil, reduciendo peso sin perder calidad relevante.\n",
      "- **API asíncrona:** Si el proveedor de verificación tarda, considera un enfoque asíncrono: permite al usuario seguir usando la app mientras espera la verificación, mostrando estado en tiempo real (“Estamos validando tus datos…”).\n",
      "- **Parallelización:** Si el proceso implica varios pasos (documento, selfie, comprobación de bases de datos), evalúa si pueden ejecutarse en paralelo.\n",
      "\n",
      "**3. Proveedor externo:**  \n",
      "- ¿Quién realiza la verificación? Algunos proveedores (Onfido, Veriff, Jumio, etc.) ofrecen APIs rápidas y SDKs optimizados para móviles.  \n",
      "- Compara tiempos de respuesta entre proveedores. Algunos tienen presencia local/regional que reduce latencia.\n",
      "\n",
      "**4. UX/UI:**  \n",
      "- Muestra una barra de progreso o feedback claro. La percepción de velocidad mejora con información transparente.\n",
      "- Permite guardar el progreso y retomar si el usuario abandona.\n",
      "- Minimiza los pasos: sólo pide los datos absolutamente necesarios.\n",
      "\n",
      "**5. Pruebas y medición:**  \n",
      "- Haz pruebas A/B con diferentes flujos y mide el tiempo real desde inicio hasta fin.\n",
      "- Analiza en qué dispositivos/OS hay más lentitud (algunas cámaras de Android, por ejemplo, pueden tardar más en procesar imágenes).\n",
      "\n",
      "**6. Backend:**  \n",
      "- Si la verificación depende de microservicios internos, revisa tiempos de procesamiento y cuellos de botella.\n",
      "- Usa colas de mensajes (RabbitMQ, Kafka) para desacoplar tareas pesadas y evitar bloquear el frontend.\n",
      "\n",
      "**7. Seguridad y cumplimiento:**  \n",
      "- Si decides procesar más datos en el edge, revisa normativas locales sobre almacenamiento y tratamiento de datos personales.\n",
      "\n",
      "¿Tienes identificada ya la parte más lenta del proceso? Si quieres, puedo ayudarte a diseñar una estrategia de medición de latencia paso a paso, o comparar opciones de proveedores rápidos para verificación KYC. ¿Qué parte te gustaría profundizar?\\n\n",
      "   Resumen actual en memoria:\n",
      "   María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\n",
      "\n",
      "María menciona que el mayor desafío técnico actual es la latencia en transacciones internacionales, por lo que están considerando edge computing y optimización de APIs. El AI reconoce la importancia de estos enfoques y recomienda estrategias técnicas como el despliegue de microservicios críticos en nodos edge regionales, caching inteligente, balanceo de carga geográfico, uso de HTTP/2 o gRPC, compresión, paginación, monitoreo de rutas lentas y optimización de conexiones. También sugiere revisar el procesamiento en tiempo real con colas de mensajes, el uso de CDN y el cumplimiento normativo en el edge. El AI pregunta en qué parte específica del flujo ocurre la mayor latencia y ofrece ayuda para analizar métricas, comparar proveedores de edge y diseñar pruebas de rendimiento para APIs.\n",
      "\n",
      "María agrega que también está trabajando en mejorar la experiencia de usuario de la app móvil, especialmente porque los usuarios se quejan de que el proceso de verificación de identidad (KYC) es muy lento. El AI resalta la importancia del onboarding rápido en fintech y recomienda analizar el flujo para localizar la demora, optimizar la carga y procesamiento local de imágenes, usar APIs asíncronas y paralelizar pasos, comparar proveedores externos especializados en KYC, mejorar el feedback y la interfaz de usuario, realizar pruebas A/B y mediciones en distintos dispositivos, optimizar el backend con colas de mensajes, y revisar normativas si se procesan datos en el edge. El AI se ofrece a ayudar a medir la latencia paso a paso y comparar opciones de proveedores KYC, preguntando en qué parte María desea profundizar.\\n\n",
      "--------------------------------------------------------------------------------\n",
      "5. Input: ¿Puedes resumir quién soy y cuáles son mis principales desafíos profesionales?...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "María González, ingeniera de software especializada en desarrollo web con React y Node.js, trabaja en una startup fintech en Madrid desde hace 3 años. El AI valora su perfil y el papel innovador de las fintech en la ciudad, preguntando por sus proyectos, tecnologías y ofreciéndose para resolver dudas sobre desarrollo web, tendencias fintech o procesos de equipo. María cuenta que su proyecto actual es una plataforma de pagos digitales en tiempo real, enfocada en alta seguridad, basada en microservicios con Docker y Kubernetes. El AI destaca los retos de escalabilidad, seguridad y resiliencia de este tipo de plataformas, explicando cómo Docker y Kubernetes ayudan y sugiriendo buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad. Además, pregunta si usan proveedores externos de pagos y qué soluciones de observabilidad han implementado, ofreciendo asesoría específica en arquitectura, seguridad y optimización del trabajo en equipo.\n",
      "\n",
      "María menciona que el mayor desafío técnico actual es la latencia en transacciones internacionales, por lo que están considerando edge computing y optimización de APIs. El AI reconoce la importancia de estos enfoques y recomienda estrategias técnicas como el despliegue de microservicios críticos en nodos edge regionales, caching inteligente, balanceo de carga geográfico, uso de HTTP/2 o gRPC, compresión, paginación, monitoreo de rutas lentas y optimización de conexiones. También sugiere revisar el procesamiento en tiempo real con colas de mensajes, el uso de CDN y el cumplimiento normativo en el edge. El AI pregunta en qué parte específica del flujo ocurre la mayor latencia y ofrece ayuda para analizar métricas, comparar proveedores de edge y diseñar pruebas de rendimiento para APIs.\n",
      "\n",
      "María agrega que también está trabajando en mejorar la experiencia de usuario de la app móvil, especialmente porque los usuarios se quejan de que el proceso de verificación de identidad (KYC) es muy lento. El AI resalta la importancia del onboarding rápido en fintech y recomienda analizar el flujo para localizar la demora, optimizar la carga y procesamiento local de imágenes, usar APIs asíncronas y paralelizar pasos, comparar proveedores externos especializados en KYC, mejorar el feedback y la interfaz de usuario, realizar pruebas A/B y mediciones en distintos dispositivos, optimizar el backend con colas de mensajes, y revisar normativas si se procesan datos en el edge. El AI se ofrece a ayudar a medir la latencia paso a paso y comparar opciones de proveedores KYC, preguntando en qué parte María desea profundizar.\n",
      "Human: ¿Puedes resumir quién soy y cuáles son mis principales desafíos profesionales?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "   Respuesta: Claro, aquí tienes un resumen de tu perfil y los principales desafíos profesionales:\n",
      "\n",
      "**Quién eres:**  \n",
      "Eres María González, ingeniera de software especializada en desarrollo web con React y Node.js. Trabajas desde hace 3 años en una startup fintech en Madrid, donde participas en proyectos innovadores relacionados con pagos digitales.\n",
      "\n",
      "**Principales desafíos profesionales:**\n",
      "\n",
      "1. **Latencia en transacciones internacionales:**  \n",
      "Tu proyecto actual es una plataforma de pagos digitales en tiempo real, donde el mayor reto técnico es reducir la latencia en pagos internacionales. Estás considerando soluciones como edge computing, optimización de APIs, caching y balanceo geográfico para mejorar la velocidad y la experiencia del usuario.\n",
      "\n",
      "2. **Experiencia de usuario en la app móvil:**  \n",
      "Estás trabajando para mejorar el proceso de verificación de identidad (KYC), que actualmente es demasiado lento para los usuarios. Buscas optimizar el flujo, procesar imágenes de manera eficiente, comparar proveedores KYC y mejorar la interfaz y los tiempos de respuesta en dispositivos móviles.\n",
      "\n",
      "**En resumen:**  \n",
      "Tu trabajo se centra en la escalabilidad, seguridad y rapidez de una plataforma fintech, enfrentando retos técnicos complejos en latencia y experiencia de usuario, especialmente en el onboarding y procesamiento de pagos internacionales. Además, te preocupas por implementar buenas prácticas en arquitectura y optimización de equipos de desarrollo.\n",
      "\n",
      "¿Te gustaría que profundicemos en algún aspecto concreto de estos desafíos?\\n\n",
      "   Resumen actual en memoria:\n",
      "   María González es ingeniera de software especializada en desarrollo web con React y Node.js, y lleva 3 años trabajando en una startup fintech en Madrid dedicada a proyectos innovadores de pagos digitales. El AI valora su perfil y le pregunta acerca de sus proyectos, tecnologías y desafíos, ofreciéndose para asesorar en temas de desarrollo web, tendencias fintech y procesos de equipo. María explica que su proyecto actual es una plataforma de pagos digitales en tiempo real, basada en microservicios con Docker y Kubernetes, enfocada en alta seguridad, escalabilidad y resiliencia. El AI destaca la importancia de estas tecnologías y sugiere buenas prácticas en seguridad, procesamiento en tiempo real y alta disponibilidad, preguntando por el uso de proveedores externos de pagos y soluciones de observabilidad.\n",
      "\n",
      "María indica que el principal reto técnico es la latencia en transacciones internacionales, considerando edge computing y optimización de APIs como posibles soluciones. El AI recomienda estrategias técnicas como despliegue regional en nodos edge, caching inteligente, balanceo geográfico, compresión, paginación, monitoreo de rutas lentas y optimización de conexiones, además de revisar el procesamiento en tiempo real, uso de CDN y cumplimiento normativo en el edge. El AI pregunta en qué parte específica ocurre la mayor latencia y ofrece ayuda para analizar métricas y diseñar pruebas de rendimiento.\n",
      "\n",
      "Además, María trabaja en mejorar la experiencia de usuario en la app móvil, especialmente en el proceso de verificación de identidad (KYC), que los usuarios consideran lento. El AI recomienda analizar el flujo, optimizar la carga y procesamiento local de imágenes, APIs asíncronas, paralelización, comparar proveedores KYC, mejorar feedback e interfaz, pruebas A/B y optimización tanto en frontend como backend, además de revisar la normativa cuando se procesa en el edge. El AI se ofrece a ayudar a medir la latencia paso a paso y comparar proveedores.\n",
      "\n",
      "Finalmente, el AI realiza un resumen del perfil profesional de María y sus principales desafíos: reducir la latencia en pagos internacionales y mejorar la verificación de identidad en la app móvil. Resalta que su trabajo se centra en la escalabilidad, seguridad y rapidez de una plataforma fintech, enfrentando retos técnicos complejos tanto en latencia como en experiencia de usuario, especialmente en el onboarding y procesamiento de pagos internacionales, preocupándose también por buenas prácticas en arquitectura y optimización del equipo de desarrollo. El AI pregunta si desea profundizar en algún aspecto concreto de estos desafíos.\\n\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo con ConversationSummaryMemory\n",
    "def ejemplo_summary_memory():\n",
    "    print(\"=== CONVERSATIONSUMMARYMEMORY ===\")\n",
    "    print(\"Resume conversaciones largas para ahorrar tokens\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria con resumen\n",
    "        memory = ConversationSummaryMemory(llm=llm)\n",
    "        \n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Simular una conversación larga con muchos detalles\n",
    "        long_inputs = [\n",
    "            \"Hola, me llamo María González, tengo 35 años y soy ingeniera de software especializada en desarrollo web con React y Node.js. Trabajo en una startup de fintech en Madrid desde hace 3 años.\",\n",
    "            \"Mi proyecto actual involucra crear una plataforma de pagos digitales que debe manejar transacciones en tiempo real con alta seguridad. Usamos microservicios con Docker y Kubernetes.\",\n",
    "            \"El mayor desafío técnico que enfrentamos es la latencia en las transacciones internacionales. Estamos considerando implementar edge computing y optimizar nuestras APIs.\",\n",
    "            \"También estoy trabajando en mejorar la experiencia de usuario de nuestra aplicación móvil. Los usuarios se quejan de que el proceso de verificación de identidad es muy lento.\",\n",
    "            \"¿Puedes resumir quién soy y cuáles son mis principales desafíos profesionales?\"\n",
    "        ]\n",
    "        \n",
    "        for i, user_input in enumerate(long_inputs, 1):\n",
    "            print(f\"{i}. Input: {user_input[:100]}...\")\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(f\"   Respuesta: {response}\\\\n\")\n",
    "            \n",
    "            # Mostrar el resumen actual\n",
    "            print(\"   Resumen actual en memoria:\")\n",
    "            print(f\"   {memory.buffer}\\\\n\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_summary_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2195b2",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory - Resumen Inteligente\n",
    "\n",
    "Esta memoria **resume** conversaciones largas en lugar de mantener todo el texto completo, ahorrando tokens significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51070f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con ConversationBufferWindowMemory\n",
    "def ejemplo_window_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERWINDOWMEMORY ===\")\n",
    "    print(\"Mantiene solo los 2 intercambios más recientes\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Crear memoria con ventana de 2 intercambios\n",
    "        memory = ConversationBufferWindowMemory(k=2)\n",
    "        \n",
    "        conversation = ConversationChain(\n",
    "            llm=llm,\n",
    "            memory=memory,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Múltiples interacciones para ver el efecto de la ventana\n",
    "        inputs = [\n",
    "            \"Mi nombre es Carlos y tengo 30 años\",\n",
    "            \"Trabajo como diseñador gráfico\",\n",
    "            \"Me gusta el café y la música jazz\",\n",
    "            \"¿Puedes recordar mi edad?\",  # Debería olvidar esto\n",
    "            \"¿Cuál es mi profesión?\"  # Debería recordar esto\n",
    "        ]\n",
    "        \n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{i}. Pregunta: {user_input}\")\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(f\"   Respuesta: {response}\\\\n\")\n",
    "            \n",
    "            # Mostrar contenido actual de la memoria\n",
    "            print(f\"   Memoria actual (k=2):\")\n",
    "            print(f\"   {memory.buffer}\\\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar ejemplo\n",
    "ejemplo_window_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697af2f4",
   "metadata": {},
   "source": [
    "## Comparación de Tipos de Memoria\n",
    "\n",
    "Veamos las diferencias entre los tipos de memoria en una misma conversación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c52e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación entre tipos de memoria\n",
    "def comparar_memorias():\n",
    "    print(\"=== COMPARACIÓN DE TIPOS DE MEMORIA ===\\\\n\")\n",
    "    \n",
    "    # Configurar diferentes tipos de memoria\n",
    "    buffer_memory = ConversationBufferMemory()\n",
    "    window_memory = ConversationBufferWindowMemory(k=2)\n",
    "    summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "    \n",
    "    # Crear conversaciones con cada tipo\n",
    "    conversations = {\n",
    "        \"Buffer (Todo)\": ConversationChain(llm=llm, memory=buffer_memory, verbose=False),\n",
    "        \"Window (k=2)\": ConversationChain(llm=llm, memory=window_memory, verbose=False),\n",
    "        \"Summary\": ConversationChain(llm=llm, memory=summary_memory, verbose=False)\n",
    "    }\n",
    "    \n",
    "    # Secuencia de inputs para probar\n",
    "    test_inputs = [\n",
    "        \"Mi nombre es Alex y estudio ingeniería informática\",\n",
    "        \"Tengo 22 años y me especializo en IA\",\n",
    "        \"Mi lenguaje favorito es Python\",\n",
    "        \"También me gusta JavaScript para desarrollo web\",\n",
    "        \"¿Cuál es mi edad y carrera?\"  # Pregunta que requiere memoria\n",
    "    ]\n",
    "    \n",
    "    # Ejecutar la misma conversación con cada tipo de memoria\n",
    "    for memory_type, conversation in conversations.items():\n",
    "        print(f\"\\\\n{'='*20} {memory_type.upper()} {'='*20}\")\n",
    "        \n",
    "        for i, user_input in enumerate(test_inputs, 1):\n",
    "            try:\n",
    "                if i < len(test_inputs):  # No mostrar la pregunta final aún\n",
    "                    response = conversation.predict(input=user_input)\n",
    "                    print(f\"{i}. Input: {user_input}\")\n",
    "                    print(f\"   Respuesta: {response[:100]}...\")\n",
    "                else:  # Pregunta final para probar memoria\n",
    "                    response = conversation.predict(input=user_input)\n",
    "                    print(f\"\\\\n{i}. PREGUNTA DE MEMORIA: {user_input}\")\n",
    "                    print(f\"   RESPUESTA: {response}\")\n",
    "                    \n",
    "                    # Mostrar estado de memoria\n",
    "                    if memory_type == \"Buffer (Todo)\":\n",
    "                        print(f\"   Memoria: {len(buffer_memory.buffer)} caracteres\")\n",
    "                    elif memory_type == \"Window (k=2)\":\n",
    "                        print(f\"   Memoria: {len(window_memory.buffer)} caracteres\")\n",
    "                    else:\n",
    "                        print(f\"   Resumen: {len(summary_memory.buffer)} caracteres\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"   Error: {e}\")\n",
    "        \n",
    "        print(\"\\\\n\" + \"-\"*60)\n",
    "    \n",
    "    print(\"\\\\n=== ANÁLISIS ===\")\n",
    "    print(\"• Buffer Memory: Recuerda todo pero consume más tokens\")\n",
    "    print(\"• Window Memory: Eficiente pero puede olvidar información importante\")  \n",
    "    print(\"• Summary Memory: Balance entre memoria y eficiencia\")\n",
    "\n",
    "# Ejecutar comparación\n",
    "comparar_memorias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818257c",
   "metadata": {},
   "source": [
    "## Chatbot Avanzado con Memoria Personalizable\n",
    "\n",
    "Implementemos un chatbot que permite al usuario elegir el tipo de memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96813f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot avanzado con memoria configurable\n",
    "def chatbot_con_memoria():\n",
    "    print(\"=== CHATBOT CON MEMORIA CONFIGURABLE ===\")\n",
    "    print(\"Tipos disponibles:\")\n",
    "    print(\"1. buffer - Mantiene todo el historial\")\n",
    "    print(\"2. window - Mantiene solo N mensajes recientes\") \n",
    "    print(\"3. summary - Resume conversaciones largas\")\n",
    "    print(\"\\\\nEscribe 'cambiar' para cambiar tipo de memoria\")\n",
    "    print(\"Escribe 'memoria' para ver el contenido actual\")\n",
    "    print(\"Escribe 'salir' para terminar\\\\n\")\n",
    "    \n",
    "    # Configuración inicial\n",
    "    memory_type = \"buffer\"\n",
    "    memory = ConversationBufferMemory()\n",
    "    conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "    \n",
    "    def crear_memoria(tipo):\n",
    "        if tipo == \"buffer\":\n",
    "            return ConversationBufferMemory()\n",
    "        elif tipo == \"window\":\n",
    "            return ConversationBufferWindowMemory(k=3)\n",
    "        elif tipo == \"summary\":\n",
    "            return ConversationSummaryMemory(llm=llm)\n",
    "        else:\n",
    "            return ConversationBufferMemory()\n",
    "    \n",
    "    print(f\"Memoria actual: {memory_type}\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\\\n🧑 Tú: \")\n",
    "        \n",
    "        if user_input.lower() == 'salir':\n",
    "            print(\"\\\\n👋 ¡Hasta luego!\")\n",
    "            break\n",
    "            \n",
    "        elif user_input.lower() == 'cambiar':\n",
    "            print(\"\\\\nTipos disponibles: buffer, window, summary\")\n",
    "            nuevo_tipo = input(\"Nuevo tipo de memoria: \").lower()\n",
    "            \n",
    "            if nuevo_tipo in ['buffer', 'window', 'summary']:\n",
    "                memory_type = nuevo_tipo\n",
    "                memory = crear_memoria(memory_type)\n",
    "                conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n",
    "                print(f\"✓ Memoria cambiada a: {memory_type}\")\n",
    "                print(\"⚠️ Historial de conversación reiniciado\")\n",
    "            else:\n",
    "                print(\"❌ Tipo no válido\")\n",
    "            continue\n",
    "            \n",
    "        elif user_input.lower() == 'memoria':\n",
    "            print(f\"\\\\n=== MEMORIA ACTUAL ({memory_type}) ===\")\n",
    "            print(f\"Contenido: {memory.buffer}\")\n",
    "            print(f\"Longitud: {len(memory.buffer)} caracteres\")\n",
    "            continue\n",
    "            \n",
    "        elif not user_input.strip():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\\\n🤖 Asistente ({memory_type}): \", end=\"\", flush=True)\n",
    "            response = conversation.predict(input=user_input)\n",
    "            print(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e}\")\n",
    "\n",
    "# Función para ejecutar el chatbot\n",
    "# chatbot_con_memoria()  # Descomenta para ejecutar\n",
    "\n",
    "print(\"💡 Descomenta la línea anterior para probar el chatbot con memoria configurable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85cfea",
   "metadata": {},
   "source": [
    "## Consideraciones Técnicas y Mejores Prácticas\n",
    "\n",
    "### Selección del Tipo de Memoria\n",
    "\n",
    "| Tipo | Cuándo Usarlo | Ventajas | Desventajas |\n",
    "|------|---------------|----------|-------------|\n",
    "| **Buffer** | Conversaciones cortas | Contexto completo | Alto consumo de tokens |\n",
    "| **Window** | Contexto reciente importante | Eficiente en tokens | Puede perder información clave |\n",
    "| **Summary** | Conversaciones muy largas | Balance eficiencia/contexto | Pérdida de detalles específicos |\n",
    "\n",
    "### Mejores Prácticas:\n",
    "\n",
    "1. **Gestión de Tokens**:\n",
    "   - Monitorea el uso de tokens regularmente\n",
    "   - Establece límites máximos para evitar costos excesivos\n",
    "   - Considera el costo vs. calidad del contexto\n",
    "\n",
    "2. **Selección Estratégica**:\n",
    "   - Usa Buffer para sesiones cortas e importantes\n",
    "   - Usa Window para conversaciones con contexto limitado\n",
    "   - Usa Summary para sesiones largas de asistencia\n",
    "\n",
    "3. **Optimización**:\n",
    "   - Limpia memoria periódicamente si es necesario\n",
    "   - Implementa estrategias híbridas según el caso de uso\n",
    "   - Considera almacenamiento persistente para memoria a largo plazo\n",
    "\n",
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Análisis de Consumo\n",
    "Implementa un sistema que monitoree y reporte el uso de tokens con diferentes tipos de memoria.\n",
    "\n",
    "### Ejercicio 2: Memoria Híbrida\n",
    "Diseña una estrategia que combine multiple tipos de memoria según el contexto.\n",
    "\n",
    "### Ejercicio 3: Persistencia\n",
    "Extiende el chatbot para guardar y cargar memoria entre sesiones.\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Importancia de la memoria** en conversaciones naturales\n",
    "2. **Tipos de memoria** y sus casos de uso específicos\n",
    "3. **Balance** entre contexto y eficiencia de tokens\n",
    "4. **Implementación práctica** con LangChain\n",
    "5. **Estrategias de optimización** para diferentes escenarios\n",
    "\n",
    "## Conclusión del Módulo IL1.1\n",
    "\n",
    "Has completado la introducción a LLMs y conexiones API. Los conceptos aprendidos:\n",
    "\n",
    "1. **APIs directas** vs **frameworks** como LangChain\n",
    "2. **Streaming** para mejor experiencia de usuario\n",
    "3. **Memoria** para conversaciones contextuales\n",
    "4. **Mejores prácticas** de seguridad y optimización\n",
    "\n",
    "### Próximos Pasos\n",
    "En **IL1.2** exploraremos técnicas avanzadas de **prompt engineering** incluyendo zero-shot, few-shot, y chain-of-thought prompting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
