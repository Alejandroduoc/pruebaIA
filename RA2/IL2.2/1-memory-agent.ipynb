{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Agentes con Memoria Conversacional en LangChain\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender por qué la memoria es crucial para crear agentes conversacionales efectivos.\n",
    "- Aprender a gestionar el historial de una conversación (`chat_history`) con el `AgentExecutor`.\n",
    "- Implementar un agente que recuerde interacciones pasadas para responder preguntas de seguimiento.\n",
    "- Entender cómo LangChain pasa el contexto de la conversación al LLM.\n",
    "\n",
    "## ¿Qué es la Memoria y Por Qué es Importante?\n",
    "\n",
    "Por defecto, los LLMs y los agentes que hemos construido hasta ahora **no tienen estado (stateless)**. Cada vez que los invocamos, procesan la solicitud como si fuera la primera vez que interactúan con nosotros. No tienen recuerdo de preguntas o respuestas anteriores.\n",
    "\n",
    "Esto es una gran limitación para crear asistentes o chatbots útiles. Un usuario espera poder hacer preguntas de seguimiento, referirse a información mencionada previamente y tener una conversación fluida. \n",
    "\n",
    "La **memoria** es el mecanismo que permite a un agente recordar interacciones pasadas. LangChain facilita enormemente la gestión de esta memoria. La forma más común de memoria es el **historial de chat (chat history)**, donde simplemente guardamos la lista de todos los mensajes de la conversación.\n",
    "\n",
    "En este notebook, veremos cómo añadir esta capacidad a nuestro agente de LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai openai wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM de LangChain configurado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configurar el idioma de Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "# Configuración del LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        openai_api_base=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        openai_api_key=os.environ.get(\"GITHUB_TOKEN\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"✅ LLM de LangChain configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error configurando el LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Herramientas y Agente (Sin Cambios)\n",
    "\n",
    "La definición de las herramientas y la creación del agente son exactamente las mismas que en el notebook anterior. La magia de la memoria no está en la definición del agente, sino en **cómo lo ejecutamos**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agente y herramientas listos.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "@tool\n",
    "def get_wikipedia_summary(query: str) -> str:\n",
    "    \"\"\"Busca en Wikipedia un tema y devuelve un resumen de 2 frases. Útil para obtener información sobre personas, lugares o conceptos.\"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception as e:\n",
    "        return f\"Ocurrió un error: {e}\"\n",
    "\n",
    "tools = [get_wikipedia_summary]\n",
    "\n",
    "# Usamos el mismo prompt de la comunidad que ya está preparado para manejar historial\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"✅ Agente y herramientas listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gestionando la Memoria Conversacional\n",
    "\n",
    "Para que el agente recuerde, necesitamos hacer dos cosas:\n",
    "\n",
    "1.  **Mantener un historial**: Crearemos una lista llamada `chat_history` para almacenar los mensajes.\n",
    "2.  **Pasar el historial en cada llamada**: El `AgentExecutor` acepta un parámetro `chat_history`. LangChain se encarga de formatear esta lista y añadirla al prompt que se envía al LLM.\n",
    "\n",
    "El formato del historial es una lista de objetos `BaseMessage` de LangChain. Los más comunes son `HumanMessage` (para el usuario) y `AIMessage` (para la respuesta del agente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# Iniciamos el historial de chat como una lista vacía\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primera Interacción: Sin Historial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'planeta Marte'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mMarte es el cuarto planeta en orden de distancia al Sol y el segundo más pequeño del sistema solar, después de Mercurio. Recibió su nombre en homenaje al homónimo dios de la guerra de la mitología romana (Ares en la mitología griega), y también es conocido como «el planeta rojo»​​ debido a la apariencia rojiza​ que le confiere el óxido de hierro predominante en su superficie.\u001b[0m\u001b[32;1m\u001b[1;3mMarte es el cuarto planeta en distancia al Sol y el segundo más pequeño del sistema solar, después de Mercurio. Es conocido como \"el planeta rojo\" debido al óxido de hierro que predomina en su superficie, y su nombre proviene del dios de la guerra en la mitología romana, Ares en la mitología griega.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 1: Marte es el cuarto planeta en distancia al Sol y el segundo más pequeño del sistema solar, después de Mercurio. Es conocido como \"el planeta rojo\" debido al óxido de hierro que predomina en su superficie, y su nombre proviene del dios de la guerra en la mitología romana, Ares en la mitología griega.\n"
     ]
    }
   ],
   "source": [
    "query1 = \"Háblame del planeta Marte\"\n",
    "\n",
    "response1 = agent_executor.invoke({\n",
    "    \"input\": query1,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Respuesta 1: {response1['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, actualizamos manualmente nuestro historial con la pregunta del usuario y la respuesta del agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historial actualizado.\n"
     ]
    }
   ],
   "source": [
    "chat_history.append(HumanMessage(content=query1))\n",
    "chat_history.append(AIMessage(content=response1[\"output\"]))\n",
    "\n",
    "print(\"Historial actualizado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segunda Interacción: Con Historial\n",
    "\n",
    "Ahora hacemos una pregunta de seguimiento. Fíjate que no mencionamos \"Saturno\", simplemente preguntamos \"¿de qué están hechos sus anillos?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mEl interior de Marte está compuesto principalmente por tres capas: el núcleo, el manto y la corteza.\n",
      "\n",
      "1. **Núcleo**: Se cree que el núcleo de Marte es metálico, compuesto principalmente de hierro, níquel y azufre. Es más pequeño en comparación con el núcleo de la Tierra y podría ser parcialmente líquido.\n",
      "\n",
      "2. **Manto**: El manto está formado por silicatos y minerales ricos en magnesio y hierro. Es menos activo que el manto terrestre, lo que explica la falta de tectónica de placas en Marte.\n",
      "\n",
      "3. **Corteza**: La corteza marciana está compuesta principalmente de basalto, un tipo de roca volcánica. Es más gruesa que la corteza terrestre y contiene minerales como feldespatos y piroxenos.\n",
      "\n",
      "Marte tiene una estructura interna más simple que la Tierra debido a su menor tamaño y menor actividad geológica.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 2: El interior de Marte está compuesto principalmente por tres capas: el núcleo, el manto y la corteza.\n",
      "\n",
      "1. **Núcleo**: Se cree que el núcleo de Marte es metálico, compuesto principalmente de hierro, níquel y azufre. Es más pequeño en comparación con el núcleo de la Tierra y podría ser parcialmente líquido.\n",
      "\n",
      "2. **Manto**: El manto está formado por silicatos y minerales ricos en magnesio y hierro. Es menos activo que el manto terrestre, lo que explica la falta de tectónica de placas en Marte.\n",
      "\n",
      "3. **Corteza**: La corteza marciana está compuesta principalmente de basalto, un tipo de roca volcánica. Es más gruesa que la corteza terrestre y contiene minerales como feldespatos y piroxenos.\n",
      "\n",
      "Marte tiene una estructura interna más simple que la Tierra debido a su menor tamaño y menor actividad geológica.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"¿Y de qué están hechos su intgerior ?\"\n",
    "\n",
    "response2 = agent_executor.invoke({\n",
    "    \"input\": query2,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "print(f\"Respuesta 2: {response2['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Funcionó! El agente entendió que \"sus anillos\" se refería a los anillos de Saturno, porque la conversación anterior estaba en su contexto. El `verbose=True` nos muestra que el agente decidió buscar en Wikipedia \"anillos de Saturno\", combinando la nueva pregunta con el historial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "Añadir memoria a un agente de LangChain es sorprendentemente sencillo, pero increíblemente poderoso. Simplemente manteniendo una lista del historial de chat y pasándola en cada invocación, transformamos un agente de una sola respuesta en un verdadero **asistente conversacional**.\n",
    "\n",
    "La clave es que los componentes de LangChain (`AgentExecutor`, los prompts de `hub`) ya están diseñados para buscar y utilizar la variable `chat_history` si se proporciona.\n",
    "\n",
    "**Limitaciones:**\n",
    "- **Gestión Manual**: En este ejemplo, actualizamos la lista `chat_history` manualmente. Para una aplicación real, querríamos encapsular esto en una clase o función.\n",
    "- **Tamaño del Contexto**: Enviar el historial completo en cada llamada puede volverse costoso y exceder el límite de tokens del modelo en conversaciones muy largas.\n",
    "\n",
    "En los próximos notebooks, exploraremos los **sistemas de memoria** que LangChain ofrece para gestionar estas limitaciones, como la memoria de búfer (`BufferMemory`) que automatiza la gestión del historial y la memoria de resumen (`SummaryMemory`) que condensa conversaciones largas para ahorrar tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
