{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sistemas de Memoria Avanzados en LangChain\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender las limitaciones de la gestión manual de la memoria.\n",
    "- Aprender a usar las clases de memoria de LangChain para automatizar el historial de chat.\n",
    "- Implementar `ConversationBufferMemory` para un historial completo.\n",
    "- Implementar `ConversationBufferWindowMemory` para limitar el tamaño del historial.\n",
    "- Implementar `ConversationSummaryMemory` para resumir conversaciones y ahorrar tokens.\n",
    "- Entender el puente hacia la planificación de agentes.\n",
    "\n",
    "## El Problema con la Memoria Manual\n",
    "\n",
    "En el notebook anterior, le dimos memoria a nuestro agente gestionando manualmente una lista `chat_history`. Aunque funcional, este enfoque tiene dos grandes inconvenientes:\n",
    "\n",
    "1.  **Gestión Tediosa**: Actualizar la lista manualmente después de cada interacción es propenso a errores y no es escalable.\n",
    "2.  **Límite de Contexto**: Enviar el historial completo en cada llamada consume tokens rápidamente. En conversaciones largas, esto puede exceder el límite de contexto del modelo, provocando errores o un alto costo.\n",
    "\n",
    "LangChain resuelve esto con un sistema de **clases de Memoria** que automatizan el proceso y ofrecen estrategias para gestionar historiales largos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Instalación y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai openai wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM de LangChain configurado.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configurar el idioma de Wikipedia\n",
    "wikipedia.set_lang(\"es\")\n",
    "\n",
    "# Configuración del LLM\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o\",\n",
    "        openai_api_base=os.environ.get(\"GITHUB_BASE_URL\"),\n",
    "        openai_api_key=os.environ.get(\"GITHUB_TOKEN\"),\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"✅ LLM de LangChain configurado.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error configurando el LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Agente y Herramientas (Sin Cambios)\n",
    "\n",
    "Reutilizamos el mismo agente y herramientas. La innovación estará en cómo gestionamos la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agente y herramientas listos.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool, create_openai_tools_agent, AgentExecutor\n",
    "from langchain import hub\n",
    "\n",
    "@tool\n",
    "def get_wikipedia_summary(query: str) -> str:\n",
    "    \"\"\"Busca en Wikipedia un tema y devuelve un resumen de 2 frases. Útil para obtener información sobre personas, lugares o conceptos.\"\"\"\n",
    "    try:\n",
    "        return wikipedia.summary(query, sentences=2)\n",
    "    except Exception as e:\n",
    "        return f\"Ocurrió un error: {e}\"\n",
    "\n",
    "tools = [get_wikipedia_summary]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"✅ Agente y herramientas listos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Automatizando con `ConversationBufferMemory`\n",
    "\n",
    "Esta es la memoria más básica. Simplemente almacena todos los mensajes en una variable (el \"buffer\") y los pasa en cada llamada. Replica lo que hicimos manualmente, pero de forma automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_10024\\442967770.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# memory_key debe coincidir con la variable de entrada en el prompt ('chat_history')\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "def chat_with_agent_buffer(query: str):\n",
    "    # Carga el historial de la memoria\n",
    "    history = memory.load_memory_variables({})[\"chat_history\"]\n",
    "    \n",
    "    # Invoca al agente con el historial\n",
    "    response = agent_executor.invoke({\"input\": query, \"chat_history\": history})\n",
    "    \n",
    "    # Guarda el nuevo par de pregunta/respuesta en la memoria\n",
    "    memory.save_context({\"input\": query}, {\"output\": response[\"output\"]})\n",
    "    \n",
    "    return response[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primera pregunta...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Marte'}`\n",
      "\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alejandro\\Documents\\GitHub\\productos_app\\Ingenier-a-de-Soluciones-con-Inteligencia-Artificial\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\Alejandro\\Documents\\GitHub\\productos_app\\Ingenier-a-de-Soluciones-con-Inteligencia-Artificial\\.venv\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3mOcurrió un error: \"Marte\" may refer to: \n",
      "Marte (planeta)\n",
      "Marte (mitología)\n",
      "hierro\n",
      "Marte (Velázquez)\n",
      "Marte, el portador de la guerra\n",
      "The Martian\n",
      "Marte (serie de televisión)\n",
      "«Marte» (canción)\n",
      "Marte (Coahuila)\n",
      "Marte (Nigeria)\n",
      "Club Deportivo Atlético Marte\n",
      "Club Deportivo Marte\n",
      "Wikcionario\n",
      "Diccionario de la Real Academia Española\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Marte planeta'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mMarte es el cuarto planeta en orden de distancia al Sol y el segundo más pequeño del sistema solar, después de Mercurio. Recibió su nombre en homenaje al homónimo dios de la guerra de la mitología romana (Ares en la mitología griega), y también es conocido como «el planeta rojo»​​ debido a la apariencia rojiza​ que le confiere el óxido de hierro predominante en su superficie.\u001b[0m\u001b[32;1m\u001b[1;3mMarte es el cuarto planeta del sistema solar y el segundo más pequeño, después de Mercurio. Conocido como \"el planeta rojo\" por el óxido de hierro en su superficie, su nombre proviene del dios romano de la guerra, Marte (Ares en la mitología griega).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 1: Marte es el cuarto planeta del sistema solar y el segundo más pequeño, después de Mercurio. Conocido como \"el planeta rojo\" por el óxido de hierro en su superficie, su nombre proviene del dios romano de la guerra, Marte (Ares en la mitología griega).\n",
      "Segunda pregunta de seguimiento...\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSí, Marte tiene dos lunas pequeñas llamadas **Fobos** y **Deimos**. Estas lunas tienen formas irregulares y se cree que son asteroides capturados por la gravedad de Marte. Fobos es la más grande y cercana al planeta, mientras que Deimos es más pequeña y está más alejada.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 2: Sí, Marte tiene dos lunas pequeñas llamadas **Fobos** y **Deimos**. Estas lunas tienen formas irregulares y se cree que son asteroides capturados por la gravedad de Marte. Fobos es la más grande y cercana al planeta, mientras que Deimos es más pequeña y está más alejada.\n"
     ]
    }
   ],
   "source": [
    "print(\"Primera pregunta...\")\n",
    "response1 = chat_with_agent_buffer(\"Háblame del planeta Marte\")\n",
    "print(f\"Respuesta 1: {response1}\")\n",
    "\n",
    "print(\"Segunda pregunta de seguimiento...\")\n",
    "response2 = chat_with_agent_buffer(\"¿Tiene alguna luna?\")\n",
    "print(f\"Respuesta 2: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, la función `chat_with_agent_buffer` ahora se encarga de cargar y guardar el historial, haciendo el proceso mucho más limpio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gestionando el Contexto con `ConversationBufferWindowMemory`\n",
    "\n",
    "Para evitar que el historial crezca sin control, podemos usar `ConversationBufferWindowMemory`. Esta memoria solo conserva un número `k` de las últimas interacciones.\n",
    "\n",
    "Esto es un buen equilibrio entre tener contexto y no sobrecargar al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_10024\\1328021610.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_window = ConversationBufferWindowMemory(k=1, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Creamos una nueva memoria, esta vez con k=1 (solo recuerda el último par de mensajes)\n",
    "memory_window = ConversationBufferWindowMemory(k=1, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "def chat_with_agent_window(query: str):\n",
    "    history = memory_window.load_memory_variables({})[\"chat_history\"]\n",
    "    response = agent_executor.invoke({\"input\": query, \"chat_history\": history})\n",
    "    memory_window.save_context({\"input\": query}, {\"output\": response[\"output\"]})\n",
    "    return response[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta 1: ¿Quién fue Albert Einstein?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Albert Einstein'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAlbert Einstein pronunciación en alemán: /ˈalbɐt ˈaɪnʃtaɪn/ ();​ (Ulm, Imperio alemán, 14 de marzo de 1879-Princeton, Estados Unidos, 18 de abril de 1955) fue un físico alemán de origen judío, nacionalizado después suizo, austriaco y estadounidense. Se le considera el científico más importante, conocido y popular del siglo XX.​​\n",
      "En 1905, cuando era un joven físico desconocido, empleado en la Oficina de Patentes de Berna, publicó su teoría de la relatividad especial.\u001b[0m\u001b[32;1m\u001b[1;3mAlbert Einstein fue un físico alemán de origen judío, nacionalizado suizo, austriaco y estadounidense, considerado el científico más importante y popular del siglo XX. En 1905, mientras trabajaba en la Oficina de Patentes de Berna, publicó su teoría de la relatividad especial, revolucionando la física.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Pregunta 2: ¿Y Niels Bohr?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'Niels Bohr'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mNiels Henrik David Bohr (Copenhague, 7 de octubre de 1885-Copenhague, 18 de noviembre de 1962) fue un físico danés que contribuyó en la comprensión del átomo y la mecánica cuántica. Fue galardonado con el Premio Nobel de Física en 1922.​​\n",
      "\n",
      "\n",
      "== Biografía ==\n",
      "\n",
      " \n",
      "\n",
      "Nació en Copenhague, hijo de Christian Bohr, un devoto luterano y catedrático de fisiología en la universidad de la ciudad, y Ellen Adler, miembro de una adinerada familia judía de gran importancia en la banca danesa y en los «círculos del Parlamento».\u001b[0m\u001b[32;1m\u001b[1;3mNiels Bohr fue un físico danés que realizó importantes contribuciones a la comprensión del átomo y la mecánica cuántica. En 1922, recibió el Premio Nobel de Física por su trabajo en este campo.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Pregunta 3: ¿De qué trataba la primera pregunta que te hice?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mLa primera pregunta que me hiciste fue: \"¿Y Niels Bohr?\". Querías información sobre él.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 3: La primera pregunta que me hiciste fue: \"¿Y Niels Bohr?\". Querías información sobre él.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pregunta 1: ¿Quién fue Albert Einstein?\")\n",
    "chat_with_agent_window(\"¿Quién fue Albert Einstein?\")\n",
    "\n",
    "print(\"Pregunta 2: ¿Y Niels Bohr?\")\n",
    "chat_with_agent_window(\"¿Y Niels Bohr?\")\n",
    "\n",
    "print(\"Pregunta 3: ¿De qué trataba la primera pregunta que te hice?\")\n",
    "response3 = chat_with_agent_window(\"¿De qué trataba la primera pregunta que te hice?\")\n",
    "print(f\"Respuesta 3: {response3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente probablemente no podrá responder a la tercera pregunta, porque con `k=1`, la interacción sobre Einstein ya fue descartada de la memoria para dar paso a la de Niels Bohr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ahorrando Tokens con `ConversationSummaryMemory`\n",
    "\n",
    "Esta es la estrategia más avanzada. En lugar de guardar los mensajes completos, utiliza un LLM para crear un resumen de la conversación a medida que avanza. En cada nueva interacción, el resumen se actualiza y se pasa como contexto.\n",
    "\n",
    "Es ideal para conversaciones muy largas donde los detalles específicos se vuelven menos importantes que el contexto general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alejandro\\AppData\\Local\\Temp\\ipykernel_10024\\1927192588.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_summary = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Esta memoria necesita un LLM para hacer los resúmenes\n",
    "memory_summary = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "def chat_with_agent_summary(query: str):\n",
    "    history = memory_summary.load_memory_variables({})[\"chat_history\"]\n",
    "    response = agent_executor.invoke({\"input\": query, \"chat_history\": history})\n",
    "    memory_summary.save_context({\"input\": query}, {\"output\": response[\"output\"]})\n",
    "    return response[\"output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta 1: Necesito organizar un viaje a Japón. ¿Cuál es la mejor época para ir?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_wikipedia_summary` with `{'query': 'mejor época para viajar a Japón'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mOcurrió un error: Page id \"mejor época para villar a japón\" does not match any pages. Try another id!\u001b[0m\u001b[32;1m\u001b[1;3mLa mejor época para viajar a Japón depende de tus intereses, pero generalmente se recomienda:\n",
      "\n",
      "1. **Primavera (marzo a mayo):** Es famosa por los cerezos en flor (sakura), especialmente a finales de marzo y principios de abril. El clima es templado y agradable.\n",
      "\n",
      "2. **Otoño (septiembre a noviembre):** Es ideal para disfrutar del follaje otoñal, con colores vibrantes en los árboles. El clima también es fresco y cómodo.\n",
      "\n",
      "Ambas estaciones ofrecen paisajes espectaculares y temperaturas agradables. Evita el verano (junio a agosto) si no te gusta el calor y la humedad, y ten en cuenta que el invierno puede ser frío, aunque tiene su encanto en las regiones montañosas.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Pregunta 2: Suena bien. ¿Qué ciudades me recomiendas visitar en un primer viaje?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mEn un primer viaje a Japón, te recomiendo visitar las siguientes ciudades, ya que ofrecen una combinación de historia, cultura, modernidad y paisajes impresionantes:\n",
      "\n",
      "1. **Tokio**: La capital de Japón es una mezcla vibrante de modernidad y tradición. No te pierdas el barrio de Shibuya, el templo Senso-ji en Asakusa, el mercado de pescado de Toyosu, y los jardines del Palacio Imperial. También puedes explorar Akihabara (para tecnología y cultura otaku) y Harajuku (para moda y cultura juvenil).\n",
      "\n",
      "2. **Kioto**: Es el corazón cultural de Japón, famoso por sus templos, santuarios y jardines tradicionales. Lugares imperdibles incluyen el Pabellón Dorado (Kinkaku-ji), el bosque de bambú de Arashiyama, el santuario Fushimi Inari Taisha con sus miles de torii rojos, y el distrito de Gion, conocido por las geishas.\n",
      "\n",
      "3. **Osaka**: Conocida por su comida callejera, su ambiente animado y su castillo histórico. Visita el área de Dotonbori para disfrutar de la gastronomía local, como el takoyaki y el okonomiyaki, y explora el Castillo de Osaka.\n",
      "\n",
      "4. **Hiroshima**: Una ciudad con una historia conmovedora. Visita el Parque Memorial de la Paz y la Cúpula de la Bomba Atómica. También puedes hacer una excursión a la isla de Miyajima para ver el famoso torii flotante del santuario Itsukushima.\n",
      "\n",
      "5. **Nara**: Ideal para una excursión de un día desde Kioto u Osaka. Es famosa por el Gran Buda en el templo Todai-ji y por los ciervos que deambulan libremente por el Parque de Nara.\n",
      "\n",
      "6. **Hakone**: Perfecta para disfrutar de aguas termales (onsen) y vistas del Monte Fuji. También puedes explorar el lago Ashi y el Museo al Aire Libre de Hakone.\n",
      "\n",
      "7. **Hokkaido (si tienes tiempo extra)**: Si viajas en invierno, Hokkaido es ideal para deportes de nieve y paisajes nevados. En verano, ofrece naturaleza exuberante y campos de flores.\n",
      "\n",
      "Estas ciudades te darán una experiencia completa de Japón, desde su modernidad hasta su rica historia y tradiciones.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Pregunta 3: ¿Sobre qué estábamos hablando?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mEstábamos hablando sobre organizar un viaje a Japón. Te mencioné las mejores épocas para visitar el país (primavera y otoño), las razones para evitar el verano y las particularidades del invierno. También te recomendé algunas ciudades para un primer viaje, como Tokio, Kioto, Osaka, Hiroshima, Nara, Hakone y Hokkaido, destacando lo que hace especial a cada una. ¿Te gustaría que profundice en alguno de estos temas?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Respuesta 3: Estábamos hablando sobre organizar un viaje a Japón. Te mencioné las mejores épocas para visitar el país (primavera y otoño), las razones para evitar el verano y las particularidades del invierno. También te recomendé algunas ciudades para un primer viaje, como Tokio, Kioto, Osaka, Hiroshima, Nara, Hakone y Hokkaido, destacando lo que hace especial a cada una. ¿Te gustaría que profundice en alguno de estos temas?\n"
     ]
    }
   ],
   "source": [
    "print(\"Pregunta 1: Necesito organizar un viaje a Japón. ¿Cuál es la mejor época para ir?\")\n",
    "chat_with_agent_summary(\"Necesito organizar un viaje a Japón. ¿Cuál es la mejor época para ir?\")\n",
    "\n",
    "print(\"Pregunta 2: Suena bien. ¿Qué ciudades me recomiendas visitar en un primer viaje?\")\n",
    "chat_with_agent_summary(\"¿Qué ciudades me recomiendas visitar en un primer viaje?\")\n",
    "\n",
    "print(\"Pregunta 3: ¿Sobre qué estábamos hablando?\")\n",
    "response3_summary = chat_with_agent_summary(\"¿Sobre qué estábamos hablando?\")\n",
    "print(f\"Respuesta 3: {response3_summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente debería poder responder correctamente, ya que la memoria no contiene los mensajes literales, sino un resumen de que el usuario está planeando un viaje a Japón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones y Transición al Módulo IL2.3\n",
    "\n",
    "Hemos visto cómo las clases de memoria de LangChain nos permiten crear agentes conversacionales mucho más robustos y eficientes, superando los desafíos de la gestión manual y el tamaño del contexto.\n",
    "\n",
    "**Resumen de Estrategias de Memoria:**\n",
    "- **`ConversationBufferMemory`**: Simple y completa. Ideal para conversaciones cortas donde cada detalle importa.\n",
    "- **`ConversationBufferWindowMemory`**: Eficiente y balanceada. Perfecta para chatbots de servicio al cliente que necesitan contexto reciente.\n",
    "- **`ConversationSummaryMemory`**: Ahorradora de tokens y escalable. La mejor opción para asistentes de largo plazo y análisis de conversaciones extensas.\n",
    "\n",
    "Con un agente que puede recordar de manera efectiva, hemos completado una pieza fundamental del rompecabezas. Sin embargo, las conversaciones del mundo real a menudo implican tareas que no se resuelven con una sola herramienta o en un solo paso.\n",
    "\n",
    "**¿Qué pasa si el agente necesita buscar información, luego calcular algo con esa información y finalmente enviar un correo electrónico?**\n",
    "\n",
    "Esto requiere que el agente pueda **planificar** una secuencia de acciones. Debe ser capaz de descomponer un objetivo complejo en pasos más pequeños y ejecutarlos en el orden correcto.\n",
    "\n",
    "Esta es la transición perfecta al **Módulo IL2.3: Planificación y Orquestación de Agentes**, donde exploraremos cómo los agentes pueden crear, seguir y adaptar planes para resolver problemas complejos y multi-paso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
